---
title: "Simple OLS Estimation"
author: "Eli Clapper"
date: "02/12/2021"
output: html_document
---

In this tutorial, an OLS linear model estimator is built from scratch that takes one numeric predictor and one outcome variable.

The linear model has the following form:
$$\boldsymbol{Y} = b_0 + \boldsymbol{b}\boldsymbol{X} + \boldsymbol{\epsilon} $$

where $\boldsymbol{Y}$ represents the vector of the outcome variable, $b_0$ is the intercept, $\boldsymbol{b}$ represents the vector of weights for each input variable in matrix $\boldsymbol{X}$ and $\boldsymbol{\epsilon}$ represents the distance between $\boldsymbol{Y}$ and the linear predictor which is: $b_0 + \boldsymbol{b}\boldsymbol{X}$.

Suppose we have a sample of 500 cases of which we observed an outcome variable Y and 1 input variable X, both drawn from a normal distribution. The variables are systematically correlated in the sense that X = Y with a certain amount of noise added to each observation. Increasing the Standard Deviation for X would lower the relationship between X and Y. 
```{r}
set.seed(6164900)
Y <- rnorm(500, 100, 15)
X <- Y + rnorm(500, 100, 15)
```

We need values for $b_0$ and $b_1$ so that $\epsilon$ is minimized. There are multiple methods to do this: Ordinary Least Squares, QR decomposition, Maximum likelihood estimation using Gradient Descent to name a few. We first consider one of the more comprehensive methods: Ordinary Least Squares.

Ordinary least squares tries to minimize the Residual Sum of Squares (RSS):
$$ 
\begin{align}   
  RSS &= \sum_{i=1}^{n}(y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2 \\
  &= \sum_{i=1}^{n}\epsilon^2_i
\end{align}
$$

With one predictor, this equation is simplified to:
$$RSS = \sum_{i=1}^{n}(y_{i} - \beta_{0} - \beta_{1}x_{i})^2$$

where $y_i$ is the outcome variable for individual i, $\beta_{1}$ is the weight for the input variable $x_i$ per individual.

Although not provided, it can be proven that to minimize RSS, the least squares estimates for $b_0$ and $b_1$ are:
$$
\begin{align}
  \hat\beta_1 &= {\frac {{n}\sum {x_{i}y_{i}}-\sum {x_{i}}\sum {y_{i}}}{{n}\sum {x_{i}^{2}}-(\sum {x_{i}})^{2}}} \\
  \hat{\beta_0} &={\bar{y}}-{\hat {\beta }}\,{\bar{x}}
\end{align}
$$


The OLS estimates would thus be:
```{r}
n <- length(Y)
b1 <- (n*sum(X*Y) - sum(X)*sum(Y)) / (n*sum(X^2) - sum(X)^2)
b0 <- mean(Y) - b1*mean(X)
c(intercept = b0, weight = b1)
```

Lets take a look at how well these estimates describe our data by plotting the line
```{r}
plot(X,Y, main = 'Relationship between X and Y')
abline(a = b0, b = b1)
```

We can see that our OLS estimates seem to give a fairly good representation of the data.
Let us obtain the predicted values, which are the values for X where Y falls on the regression line:
$\hat{\boldsymbol{Y}} = b_0 + b_1\boldsymbol{X}$. We also define some metrics on which we can evaluate the predicted values. We look at RMSE, Mean Absolute Deviation, Maximal and minimal deviation. 
```{r}
y_pred <- b0 + b1*X
source('../Created_Functions.R')
```

lets see how OLS did
```{r}
metrics(Y, y_pred)
```

Not bad. Also if we obtain the estimates using the `lm` function, we get the same results for the estimates. Note that the `lm` function uses QR decomposition to find estimates and not OLS. Still the metrics of performance are similar.
```{r}
fit.lin <- lm(Y~X)
metrics(Y, fit.lin$fitted.values)
```


#Performance using cross-validation
Now we can split X and Y up in training and testing data. Then we estimate the parameters on the training data and use the parameters on the input matrix of the testing data to obtain estimates for the testing outcome. We finally will compare them to the observed test data outcome.
```{r}
set.seed(6164900)
split <- sample(1:length(X), size = floor(length(X)*0.8)) #80% of observed variables in X will be used as training data

#input matrices 
train_input <- X[split]
test_input <- X[-split]

#outcome vectors
train_outcome <- Y[split]
test_outcome <- Y[-split]

#estimate parameters on training data
n <- length(train_outcome)
b1 <- (n*sum(train_input*train_outcome) - sum(train_input)*sum(train_outcome)) / 
  (n*sum(train_input^2) - sum(train_input)^2)
b0 <- mean(train_outcome) - b1*mean(train_input)

#apply parameter estimates on test input.
y_pred_test <- b0 + b1*test_input

#obtain metrics
metrics(test_outcome, y_pred_test)

```

It can be seen that performance is slightly worse than when predicted the outcome on the same data that the model was estimated. The amount of variance explained in the outcome is now a bit lower and we see that the RMSE is somewhat higher. This is to be expected however, because in the cross-validation example we predicted the outcome on data that was *not* used to estimate the model parameters.

# Outro
In the next file we will take a look at how OLS works when we have multiple variables in the model to explain the outcome.