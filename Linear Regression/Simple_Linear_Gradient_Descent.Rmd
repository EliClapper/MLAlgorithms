---
title: "Gradient Descent Estimation"
author: "Eli Clapper"
date: "02/12/2021"
output: html_document
---

```{r}
set.seed(6164900)
Y <- rnorm(500, 100, 15)
X <- Y + rnorm(500, 100, 15)
```

Another way of finding $b_0$ and $b_1$ is to use a gradient descent algorithm.
This implies we need the gradient of the RSS which are the derivatives of the RSS with respect to $b_0$ and $b_1$.
Where this gradient is lowest, the error is minimized. This concept is explained in `Maximum_Likelihood.Rmd`. Gradient Ascent/Descent is an iterative method where we continually put in different values for $b_0$ and $b_1$ until we find the lowest value we can find. This concept is explained in `ML_Gradient_Ascent.Rmd`

So I just provide the derivatives here for simple linear regression and create the algorithm.
$$
\begin{align}
  \frac{\partial f(\beta_n|Y,X)}{\partial\beta_0} &= -2*\frac{\sum^n_{i=1}(y_i - \beta_0+\beta_1x_i)} {n} \\
  \frac{\partial f(\beta_n|Y,X)}{\partial\beta_1} &= -2*\frac{\sum^n_{i=1}x_i*(y_i - \beta_0+\beta_1 x_i)} {n} \\
\end{align}
$$

the function below produces same stimate for b1 as `lm()`, but intercept seems random
```{r}
GradientDescent <- function(Y, X, l_rate = 1E-6, thresh = 10, max_iter = 10000){
  b0 <- runif(1) #set initial values
  b1 <- runif(1) #set initial values
  y_pred <- b0 + b1*X
  MSE <- mean((Y-y_pred)^2) #set initial MSE with random values
  iter <- 0
  
  while(MSE > thresh){
      Db0 <- mean(-2*(Y - y_pred)) #derivative w.r.t b0
      Db1 <- mean(-2*X*(Y - y_pred)) #derivative w.r.t b1

      b0 <- b0 - l_rate*Db0 #updated value
      b1 <- b1 - l_rate*Db1 #updated value
      
      y_pred <- b0 + b1*X #update predicted values
      MSE <- mean((Y-y_pred)^2) #update MSE
      iter <- iter + 1 #count iterations up
      
      if(MSE == Inf | MSE == -Inf){
        stop('No convergence, try a smaller learning rate such as 0.000001')
      }
      
      if(iter == max_iter){
        print('max iteration reached')
        return(c(b0,b1))
      }
   }
  print('solution found:')
  return(c(b0,b1))

}

```

Let's see how the line fits if we use the function
```{r}
estimates <- GradientDescent(Y,X)
plot(X,Y)
abline(estimates[1], estimates[2])
```
Seems like a pretty good fit.

Let see how the hold up against the `lm` function
```{r}
estimates
coef(lm(Y~X))
```
The coefficient for the predictor, the slope, is very similar. The intercept is different, but this does not matter, because the point where X crosses Y is very far away and variation in the intercept does not affect the final regression line alot.

#evaluate using cross-validation
```{r}
set.seed(6164900)
split <- sample(1:length(X), size = floor(length(X)*0.8)) #80% of observed variables in X will be used as training data

#input matrices 
train_input <- X[split]
test_input <- X[-split]

#outcome vectors
train_outcome <- Y[split]
test_outcome <- Y[-split]

#estimate parameters on training data
n <- length(train_outcome)
estimates <- GradientDescent(train_outcome, train_input)

#apply parameter estimates on test input.
y_pred_test <- estimates[1] + estimates[2]*test_input

#obtain metrics
source("../Created_Functions.R")
metrics(test_outcome, y_pred_test)

```

