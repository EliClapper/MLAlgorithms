---
title: "Multi-linear Gradient Descent"
author: "Eli Clapper"
date: "19/12/2021"
output: html_document
---

```{r}
set.seed(6164900)
Y <- rnorm(100, 0, 3)
X <- cbind(replicate(3, rnorm(100)))
```


We have seen Gradient Ascent/Descent a couple times now, So ill just show the derivatives of the weights and the intercept and create an algorithm afterwards.

$$
\begin{align}
  \frac{\partial f(\beta_n|Y,X)}{\partial\beta_0} &= -2*\frac{\sum^n_{i=1}(y_i - \beta_0+\boldsymbol{\beta} \boldsymbol{X}_{i})} {n} \\
  \frac{\partial f(\beta_n|Y,X)}{\partial\beta_j} &= -2*\frac{\sum^n_{i=1}x_{ij}*(y_i - \beta_0+\boldsymbol{\beta} \boldsymbol{X}_{i})} {n} \\
\end{align}
$$
```{r}
GD_Multi <- function(Y, x, l_rate = 1E-3, thresh = 1, maxit = 5000){
  
  n <- length(Y)
  Nx <- ifelse(is.null(ncol(x)), length(x), nrow(x))
  nx <- ifelse(is.null(ncol(x)), 1, ncol(x))    #number of predictors
  
  weights <- matrix(NA, maxit, nx+1)                    #create memory for the weights
  colnames(weights) <- c("b0", sprintf('b%d', 1:nx))    #give names
  weights[1,] <- replicate(nx+1, runif(1))              #set initial values

  y_pred <- weights[1,1] + rowSums(t(t(x)*weights[1,2:(nx+1)])) #calculate fitted values using initial weights
  res <- Y-y_pred
  MSE <- mean((res)^2)                                  #set initial MSE with random values
  iter <- 1                                             #set counter
  
  Dbs <- matrix(NA, maxit, nx+1)                        #set matrix for derivatives
  
  while(MSE > thresh){
      Dbs[iter, 1] <- -2*mean(res) #derivative w.r.t b0
      for(i in 2:(nx+1)){                   #derivative w.r.t weights
        Dbs[iter, i] <- -2* mean(x[, (i-1)]*res)
      }

      #this is the same as above.
       # Dbs[iter,1] <- (2/n) * sum(-(res))
       # for(i in 2:(nx+1)){                   
       #   Dbs[iter, i] <- (2/n) * sum(-x[,i-1]*res)
       # }
      
      weights[(iter+1),1] <- weights[iter,1] - l_rate*Dbs[iter,1] #update intercept
      weights[(iter+1),2:(nx+1)] <- weights[iter,2:(nx+1)] - l_rate*Dbs[iter, 2:(nx+1)] #update regression coefficient
      
      y_pred <- weights[iter,1] + rowSums(t(t(x)*weights[iter,2:(nx+1)])) #update predicted values
      
      res <- Y-y_pred
      MSE <- mean((res)^2) #update MSE
      
      
      iter <- iter + 1 #count iterations up

      if(MSE == Inf | MSE == -Inf){
        stop('No convergence, try a smaller learning rate such as dividing the current learning rate by 10')
      }
      
      if(iter == maxit){
        print('max iteration reached')
        return(weights)
      }
   }
  print('solution found')
  return(weights)

}

set.seed(6164900)
estimates <- GD_Multi(Y = Y, x = X, maxit = 10000, l_rate = 1E-3)

```
lets test it against OLS estimates
```{r}
source("../Created_Functions.R")
apply(na.omit(estimates), 2, mean)
OLSests <- OLS(Y,X)
OLSests$estimates[,1]

```

```{r}
plot(estimates[,1], col = 'blue', type = 'l', xlab = "Iteration", ylab = 'estimates', main = 'Traceplot Estimates', 
     ylim = -0.5:0.5)
lines(estimates[,2], col = 'red', type = 'l')
lines(estimates[,3], col = 'green', type = 'l')
lines(estimates[,4], col = 'orange', type = 'l')

for(i in 1:4){
  abline(h = OLSests$estimates[i,1], lty = 3, col = 'black')
}


legend(6000, 0, legend=c(sprintf("b%d", 1:4), 'OLS estimates'),
       col=c("blue", "red", "green", "orange", "black"), lty=c(1,1,1,1,3), cex=1)

```


