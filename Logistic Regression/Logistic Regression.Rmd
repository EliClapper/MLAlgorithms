
In Logistic regression it does not make sense to draw a line through the relationship between X and Y, because Y is either 1 or 0 Drawing a regression line would give real values that can also be decimals.
```{r}
set.seed(6164900)
Y <- rep(0:1, each = 50) #simulate from binomial with probability 0.5
X1 <- c(rnorm(50, -0.5, 1), rnorm(50, 0.5, 1))

```

```{r}
source("../Created_Functions.R")
ols <- OLS(Y, X1) #retrieved from Linear_model.Rmd
plot(X1,Y) # someone with score 0 on X
b0 <- ols$estimates[1,1]
b1 <- ols$estimates[2,1]
abline(b0, b1)
# fit <- glm(Y~X, family = 'binomial')
# predval <- ifelse(fit$fitted.values >= 0.5, 1,0)
# mean(Y == predval)
# summary(fit)

```
Plugging in OLS estimates gives us a line through the data that would predict very badly, because the outcome is binary and not continuous. So, what essentially is done, is we convert Y into a continuous probability instead of a binary outcome.

This is done using a sigmoid function that will never go below 0 or above 1. We thus do not model Y itself, but $P(Y = 1|X)$, denoted as p(x). this makes Y continuous. This probability can be modeled with a sigmoid function of X:
$$
p(x) = \frac{e^{\beta_0 + \beta_1X}}{1+e^{\beta_0 + \beta_1X}}
$$


But this makes it difficult to estimate $\beta_0$ and $\beta_1$. 
And so we rewrite the model like so:
$$
\frac{p(x)}{1-p(x)} = e^{\beta_0 + \beta_1X}
$$
and even further to obtain the so called logit function:
$$
\log(\frac{p(x)}{1-p(x)}) = \beta_0 + \beta_1X
$$
and we arrive back at a linear function. However, this time the function models the log odds of p(x) and not p(x) itself.


This makes it easier to estimate $\beta_0$ and $\beta_1$ to make the model fit.
We estimate them with maximum likelihood which is a method that can be applied for non-linear methods to estimate parameters.
MLE requires a few steps:
1.) Impose Distribution on Y
2.) Find likelihood of Y for one data point
3.) Given independent data points, likelihood is product of individual likelihoods
4.) Take the log of the likelihood
5.) maximize loglikelihood w.r.t parameters to be estimated ($\beta_0$ and $\beta_1$ )

1.) Because Y is either 1 or 0, we should treat is as a random variable drawn from a Bernoulli distribution with a succes probability for very case:
$$
S_i = \frac{1}{1+e^{-(\beta_0 + \beta_1X_i)}}
$$

2.) for every case, the $p(Y=y|X=\boldsymbol{x})$ = likelihood =
$$
S_i^{y_i} * (1- S_i^{(1-y_i)})
$$
3._If we can assume independent observations, the likelihood of this function is the product of this function over all cases.
so:
$$
\begin{align}
L(\boldsymbol{\beta}) &= \prod^n_{i=1}p(Y=y_i|X=\boldsymbol{x_i}) \\
&= \prod^n_{i=1}S_i^{y_i} * (1- S_i)^{(1-y_i)}
\end{align}
$$
4.) To find maximum likelihood estimates, we need to take the derivative of this function at some point. It is easier to do that for the log of the likelihood. And so, the log of the likelihood is:
$$
LL(\boldsymbol{\beta}) = \sum^n_{i=1}y_i*(\log(S_i) + (1-y_i))*\log(1-S_i)
$$
We need to find values for $\beta_0$ and $\beta_1$ that maximize this function. Note that $\beta_0$ and $\beta_1$ are in the term $S$.

However, conventionally, we use the cost function that follows from this formula, which is:
$$
C(\boldsymbol{\beta}) = -\frac{\sum^n_{i=1}y_i*(\log(S_i) + (1-y_i))*\log(1-S_i)}{n}
$$

The easiest would be to set the derivative of this function to 0 to find the estimates. Because we have to estimate 2 terms which are dependent on each other, we cannot find solutions for both. This is why we need an optimization algorithm to find a solution. The algorithm we will use is the gradient descent algorithm. It makes use of partial derivatives pf $C(\boldsymbol{\beta})$ w.r.t a certain parameter and iteratively puts in new values for $\boldsymbol{\beta}$ until it is minimized. However, the input is not random, but depends on the previous value of $\beta$

It can be shown that the partial derivative of the LL function w.r.t a certain parameter for the bernoulli case is:
$$
\frac{\partial C(\boldsymbol{\beta})}{\partial \beta_j} = \frac{\sum^n_{i=1}(y_i - S_i)x_{ij}}{n}
$$ 

lets first generate multivariate data
```{r}
X <- cbind(X1, rnorm(100))
```

```{r}
GradDescBern <- function(Y, X, rate, thresh, maxit){
  
  n <- length(Y)
  sigmoid <- function(x){
    return(1/(1+exp(-x)))
  }
  
  X <- cbind(rep(1,n), X)
  ests <- cbind(replicate(ncol(X), rnorm(1,0,0.001))) #inital values
  
  y_pred <- sigmoid(X%*%ests) #inital predictions
  Y <- as.matrix(Y)
  
  iter <- 0
  
  pred_rate <- mean(Y == ifelse(y_pred >= 0.5, 1, 0)) #prediction rate

  while(pred_rate < thresh){
    
    y_pred <- sigmoid(X%*%ests)
    dw <- (t(X)%*%(y_pred - Y))/n #derivatives w.r.t parameter
    ests <- ests - rate*dw
    
    
    pred_rate <- mean(Y == ifelse(y_pred >= 0.5, 1, 0))
    iter <- iter+1

    if(iter == maxit){
      print('maximum iterations reached')
      return(list(dw, pred_rate))
    }
  }
  print(paste0('solution found after ', iter, ' iterations'))
  return(list(dw, pred_rate))
}  

GradDescBern(Y,X,rate=0.0001, 0.80, 20000) #for some reason, it converges on 0.52 instead of 0.69
summary(glm(Y~X, family = "binomial"))

```