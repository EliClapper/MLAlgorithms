A widely used theorem used in probability theory is Bayes Theorem:
$$
  P(A|B) = \frac{P(B|A)*P(A)}{P(B)}
$$
The theorem states that the conditional probability of A given B is the quotient of the product of the conditional probability of B given A and the marginal probability of A with the marginal probability of B.

In numerous classification problems we are interested in the conditional probability of an observation being in class $k$ given the predictor space $\boldsymbol{X}$. The theorem is then rewritten as:
$$
  P(Y = k| X = x) = \frac{f_k(x)*\pi_k}{\sum^N_{l=1}\pi_lf_l(x)}
$$

where $P(Y = k| X = x)$ is the posterior probability, $k$ is a certain class, $\pi_k$ is the overall/prior probability of being in class K, $f_k(x)$ is the probabilty density function [PDF] of $\boldsymbol{X}$ given class K and $\sum^N_{l=1}\pi_lf_l(x)}$ is the likelihood function.

some parameters are easily calculated. $\pi_k$ for example can be estimated in the sample by calculating the proportion of cases being in class $k$. But where a lot of classifiers differ, such as Linear Discriminant Analysis and Quadratic Discrimant Analysis is the way how $f_k(x)$ is estimated. In Naive Bayes, $f_k(x)$ is the product of all $k$ PDFs.

