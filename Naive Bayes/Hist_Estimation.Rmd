---
title: "Histogram Estimation"
author: "Eli Clapper"
date: "28-12-2021"
output: html_document
---

The theory behind this method has been discussed in `Naive_Bayes_Theory`. 

very briefly, this method classifies a case for which the marginal histograms of the predictors have the highest proportion cases in the same bin as the case. if p = 1, it looks as follows:
```{r}
set.seed(6164900)
Nk <- 250
K <- 2

Y <- rep(1:K, each = Nk)
X <- c(rpois(Nk, 2) + rnorm(Nk, 0, 0.01), rpois(Nk, 4) + rnorm(Nk, 0, 0.01))
df <- cbind(Y,X)

g1 <- hist(X[1:Nk],  breaks = 25, plot = F)
g2 <- hist(X[Nk:length(X)], breaks = 25, plot = F)
plot(g1, col = rgb(0,1,0,0.5), main = "Histogram of X for both groups",
     xlab = "X")
plot(g2, col = rgb(1,0,0,0.5), add = T)
abline(v = 1.86, lty = 2, lwd = 3, col = "blue")
legend(5, 35, fill = c("green", "red", "blue"),
       legend = c("group 1", "group 2", "x = 1.86"))

```

We would classify $x_i = 1.86$ as a member of group 1 because the proportion of cases in the same bin as $x_i$ is highest for group 1. Saving the histogram in an object allows the retrieval of the proportion of cases in a specific bin. Note, that in the code above, the histogram for group 1 and 2 is saved in object `g1` and `g2` respectively. So lets make R classify the case itself.

We need to know in which bin a specific case falls, what the proportion cases in that bin is and in which group that proportion is highest. 

```{r}
findprop <- function(x, histo){
  
  #put value between bins it is located by sorting the breaks with the value     appended to it.
  breaks <- sort(c(histo[["breaks"]], x)) 
  
  #obtain index of that value subtracted by one, because there is always one more break than the length of the proportions per bin.
  index <- which(breaks == x) - 1 
  
   #find proportion of cases in the bin x belongs too
  prop <- histo[["density"]][index]
  
  #if prop = NA or index == 0, then the new x value is outside the range of the observed X and so there are no cases in that bin.
  prop <- ifelse(index == 0, NA, prop)
  prop <- ifelse(is.na(prop), 0, prop) 
  
  #finally, if x precisely falls on a bin boundary, then prop has 2 values so if that is the case, let us take the mean.
  prop <- ifelse(length(prop) > 1, mean(prop), prop)
  
  return(prop)
}

findprop(1.2, g1)
findprop(1.86, g2)

```
The proportion, which is $f_k(x)$, is highest in group 1, so we assign $x_i = 1.86$ to group 1. 

now we code a function that runs it for the entire df to get proportions
```{r}
findprops <- function(df, breaks){
  K <- length(table(df[,1]))
  
  groups <- list() #create memory
  for(i in 1:K){ #save all groups into separate df
    groups[[i]] <- df[df[,1] == i,]
  }
  
  hists <- lapply(groups, function(x){
    hist(x[,2], plot = F, breaks = breaks)
  })
  
  props <- 
    lapply(hists, function(x){
      for(i in 1:nrow(df)){
        findprop(i,x)
      }})
    
  
}
```


Also, let's make the algorithm take multiple predictors. Remember from `Naive_Bayes_Theory.Rmd` that the form of the NB takes on:
$$
  P(Y = k| X = x) = \frac{\prod^{n_k}_{j=1}f_{jk}(x_j)*\pi_k}{\sum^K_{l=1}\pi_l\prod^{n_j}_{j=1}f_{jl}(x_j)}
$$
So for a case, we just take the product of all proportions obtained from all marginal histograms and multiply it with the prior probability of being in that class. Let's first generate data with p > 1.
```{r}
set.seed(6164900)
Nk <- 250
K <- 2

Y <- rep(1:K, each = Nk)
X <- cbind(c(rpois(Nk, 2) + rnorm(Nk, 0, 0.01), 
             rpois(Nk, 4) + rnorm(Nk, 0, 0.01)),
        c(rnorm(Nk, -0.5), rnorm(Nk, 0.5, 1.5)))
df <- cbind(Y,X)
```


Lets code. First we create a function that gets the product of the proportions for the predictors. **this function makes it impossible to run group 1 on the histogram of group 2**
```{r}
prodprops <- function(df, breaks = 25){
  P <- ifelse(is.null(ncol(df)), 1, ncol(df[,2:ncol(df)])) #number of predictors

  hists <- apply(df[,2:ncol(df)], 2, function(x){ #get histograms for each predictor
    hist(x, breaks = breaks, plot = F)
  })
  
  #get proportion for all Predictors, but in same group!
  #we need it for both groups.
  props <- matrix(0, nrow(df), P) #storage
  for(p in 2:(P+1)){ #for every predictor
    propp <- c() #storage
    for(i in 1:nrow(df)){ #for every case
      val <- findprop(df[i,p], hists[[(p-1)]]) #get value for case - predictor
      propp <- append(propp, val) #append it to the storage
    }
    props[,(p-1)] <- propp #append vector of values for 1 predictor, but all cases to the storage.
  }
  
  return(apply(props, 1, prod)) #return the product of the proportions
}
head(prodprops(df, 25))
```

```{r}
HistEst <- function(df, breaks = 25){
  
  N <- nrow(df)
  K <- length(table(df[,1])) #number of groups
  P <- ifelse(is.null(ncol(df)), 1, ncol(df[,2:ncol(df)])) #number of predictors

  #create separate df for each group
  groups <- list() #create memory
  for(i in 1:K){ #save all groups into separate df
    groups[[i]] <- df[df[,1] == i,]
  }
  
  probs <- sapply(groups, function(x){nrow(x)}) / N #proportions nk of N
  
  #list with K*P elements where each K elements are of the Pth predictor So if 2 classes and 2 predictors, than element 1 and 2 are for predictor 1.
  hists <- unlist(lapply(groups, function(y){
    apply(y[,2:ncol(df)], 2, function(x){ #get histograms for each predictor
      hist(x, breaks = breaks, plot = F)
    })
  }), recursive = F)
  
  for(j in 1:P){
    for(i in 1:nrow(df)){
      findprop(df[i,(j+1)], hists)
    }
  }

  

  #apply prodprop() to all classes
  prods <- lapply(groups, function(x){
    prodprops(x, breaks)
  })

  #####
  #We need to run findprop on all histograms, the difficult part is that the histograms must only be used on the predictor in df that corresponds to that histogram.
  #####
  
} 
```

